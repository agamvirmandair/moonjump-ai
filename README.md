# MoonJump-AI  ğŸš€  
### A Custom 2D Gymnasium Environment with Proximal Policy Optimization (PPO)

---

## ğŸ“Œ Project Overview  
This project explores reinforcement learning by creating a custom 2D game environment in **Gymnasium** and training agents using **Proximal Policy Optimization (PPO)**.  

The game, **MoonJump**, challenges an agent-controlled robot to dodge incoming obstacles of varying speeds, sizes, and distances.  

**Key contributions:**  
- ğŸ•¹ï¸ Custom Gymnasium environment (`MoonJump-v0`) built with Pygame  
- ğŸ¤– Baseline PPO training using Stable Baselines3  
- ğŸ§  From-scratch PPO implementation in PyTorch for deeper understanding  
- ğŸ“Š Performance comparison between SB3 and custom PPO agent  

<br>

---

## ğŸ® Environment Design  

**Action Space:**  
- `Discrete(2)` â†’ `0 = do nothing`, `1 = jump`  

**Observation Space (vector of floats):**  
- Robotâ€™s vertical position  
- Robotâ€™s vertical velocity  
- Distance to nearest obstacle  
- Height of nearest obstacle  
- Obstacleâ€™s horizontal speed  

<br>

---

## ğŸ† Reward Structure  

- **+0.1 + score Ã— 0.01** â†’ survival reward per frame  
- **+1** â†’ for every obstacle avoided  
- **âˆ’5 + (PLAYER_X âˆ’ obstacle_x) Ã— 0.01** â†’ penalty on collision, with small progress bonus  

ğŸ”¹ **Details:**  
- **âˆ’5 penalty** â†’ strong negative signal on crash  
- **Progress bonus** â†’ higher reward as agent passes obstacles  
- **Do nothing & crash** â†’ strong penalty  
- **Jump & survive longer** â†’ smaller penalty  

ğŸ‘‰ This balance encourages **long-term survival strategies** rather than random jumping.  

**Termination:**  
- Robot collides with an obstacle  

<br>

---

## âš™ï¸ Training Setup  

**Algorithms:**  
- PPO (baseline + custom)  

**Libraries:**  
- Gymnasium  
- Stable Baselines3  
- PyTorch  

**Hyperparameters:**  
- Learning rate: `3e-4`  
- Î³ (discount factor): `0.99`  
- n-steps: `2048`  
- Clip range: `0.2`  

**Custom PPO Highlights:**  
- Advantage estimation  
- Policy clipping  
- Separate actor-critic networks  
- Training loop built from scratch with PyTorch  

<br>

---

## ğŸ“Š Results  

**Stable Baselines3 PPO**  
- Best average score â‰ˆ **500 (max_len)**  
- More stable & sample efficient  

<p align="center">
  <img width="1332" height="443" alt="sb3_ppo_graph" src="https://github.com/user-attachments/assets/e1e57bc4-71b8-4dee-a9f9-00a4cae8ffa0" />
</p>  

**Custom PPO**  
- Best average score â‰ˆ **100**  
- Slower, but shows correct learning dynamics  

<p align="center">
  <img width="640" height="480" alt="Agent1_ppo_graph" src="https://github.com/user-attachments/assets/db48c1b9-79b8-462c-8603-95572a59e5ab" />
</p>  

---


## ğŸ¥ Demo

![MoonJump_gif](https://github.com/user-attachments/assets/3253f356-f594-4bbe-adcb-6e417be34793)


